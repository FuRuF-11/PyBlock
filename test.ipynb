{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import dataset,dataloader \n",
    "import torch.optim as optim\n",
    "from Transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"d_model\": 50,\n",
    "    \"hidden_size\": 100,\n",
    "    \"sentence_length\": 50,\n",
    "    \"layer_size\": 8,\n",
    "    \"head_num\": 8,\n",
    "    \"dropout\": 0.3,\n",
    "    \"lr\":0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ones() received an invalid combination of arguments - got (float, float), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m example\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m Tran\u001b[38;5;241m=\u001b[39m\u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m Tran(example)\n",
      "File \u001b[1;32mc:\\Users\\furuf\\Desktop\\workspace\\PyBlock\\Transformer\\transformer.py:40\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, d_model, layer_size, head, pad, dropout, max_length)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28msuper\u001b[39m(Transformer,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m=\u001b[39mEncoder(d_model,layer_size,head,dropout,max_length)\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m=\u001b[39m\u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlayer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(d_model,d_model,bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad\u001b[38;5;241m=\u001b[39mpad\n",
      "File \u001b[1;32mc:\\Users\\furuf\\Desktop\\workspace\\PyBlock\\Transformer\\decoder.py:34\u001b[0m, in \u001b[0;36mDecoder.__init__\u001b[1;34m(self, d_model, layer_size, head, dropout, max_length)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN\u001b[38;5;241m=\u001b[39mlayer_size\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition\u001b[38;5;241m=\u001b[39mCosinPosition(d_model,max_length,dropout)\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m=\u001b[39mcloneLayers(\u001b[43mDecoderBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m,layer_size)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNorm\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mLayerNorm(d_model)\n",
      "File \u001b[1;32mc:\\Users\\furuf\\Desktop\\workspace\\PyBlock\\Transformer\\decoder.py:15\u001b[0m, in \u001b[0;36mDecoderBlock.__init__\u001b[1;34m(self, d_model, head, dropout)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,d_model,head\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28msuper\u001b[39m(DecoderBlock,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention1\u001b[38;5;241m=\u001b[39m\u001b[43mCasualSelfAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention2\u001b[38;5;241m=\u001b[39mMultiHeadAttention(d_model,head,dropout)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeedforward\u001b[38;5;241m=\u001b[39mMLP(d_model,dropout)\n",
      "File \u001b[1;32mc:\\Users\\furuf\\Desktop\\workspace\\PyBlock\\Transformer\\attention.py:79\u001b[0m, in \u001b[0;36mCasualSelfAttention.__init__\u001b[1;34m(self, d_model, head, sentence_length, dropout)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,d_model,head,sentence_length,dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28msuper\u001b[39m(CasualSelfAttention)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\\\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m,torch\u001b[38;5;241m.\u001b[39mtril(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43msentence_length\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39m\\\n\u001b[0;32m     80\u001b[0m         view(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,sentence_length,sentence_length))\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention\u001b[38;5;241m=\u001b[39mSelfAttention(d_model\u001b[38;5;241m=\u001b[39md_model,head\u001b[38;5;241m=\u001b[39mhead,dropout\u001b[38;5;241m=\u001b[39mdropout)\n",
      "\u001b[1;31mTypeError\u001b[0m: ones() received an invalid combination of arguments - got (float, float), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "example=torch.randn(64,50,50)\n",
    "Tran=Transformer(d_model=50)\n",
    "Tran(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
