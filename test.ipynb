{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import dataset,dataloader \n",
    "import torch.optim as optim\n",
    "from Transformer import Transformer\n",
    "from Transformer import Encoder,EncoderBlock\n",
    "from Transformer import Decoder,DecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    # head number of Multi-head attention\n",
    "    \"head_num\": 8,\n",
    "    \"batch_size\": 64,\n",
    "    # source sentence length\n",
    "    \"src_length\": 50,\n",
    "    # traget sentence length\n",
    "    \"trg_length\": 40,\n",
    "    # layer size of encoder and decoder\n",
    "    \"layer_size\": 8,\n",
    "    # dropout rate\n",
    "    \"dropout\": 0.3,\n",
    "    # learning rate\n",
    "    \"lr\":0.001\n",
    "}\n",
    "# input word embedding size which should be the times number of head_num\n",
    "config[\"d_model\"]=config[\"head_num\"]*50\n",
    "# hidden_size of hidden layers in MLP\n",
    "config[\"hidden_size\"]=2*config[\"d_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    '''\n",
    "    seq_q: [batch_size, seq_len]\n",
    "    seq_k: [batch_size, seq_len]\n",
    "    seq_len could be src_len or it could be tgt_len\n",
    "    seq_len in seq_q and seq_len in seq_k maybe not equal\n",
    "    '''\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1).float()  # [batch_size, 1, len_k], True is masked\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 40, 50])\n",
      "tensor([[1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(config[\"batch_size\"],config[\"src_length\"],config[\"d_model\"])\n",
    "b=torch.ones(config[\"batch_size\"],config[\"trg_length\"],config[\"d_model\"])\n",
    "c=torch.zeros(config[\"d_model\"])\n",
    "a[:,-2:]=c\n",
    "b[:,-3:]=c\n",
    "tmp1=(a.sum(dim=2)==0)\n",
    "tmp2=(b.sum(dim=2)==0)\n",
    "# mask=get_attn_pad_mask(tmp1,tmp1)\n",
    "# mask=get_attn_pad_mask(tmp2,tmp2)\n",
    "mask=get_attn_pad_mask(tmp2,tmp1)\n",
    "print(mask.size())\n",
    "print(mask[0])\n",
    "# Tran=Transformer(config)\n",
    "# res=Tran(a,b)\n",
    "# print(res.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
