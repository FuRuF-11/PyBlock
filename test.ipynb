{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import dataset,dataloader \n",
    "import torch.optim as optim\n",
    "from Transformer import Transformer\n",
    "from Transformer import Encoder,EncoderBlock\n",
    "from Transformer import Decoder,DecoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    # head number of Multi-head attention\n",
    "    \"head_num\": 8,\n",
    "    \"batch_size\": 64,\n",
    "    # source sentence length\n",
    "    \"src_length\": 50,\n",
    "    # traget sentence length\n",
    "    \"trg_length\": 40,\n",
    "    # layer size of encoder and decoder\n",
    "    \"layer_size\": 8,\n",
    "    # dropout rate\n",
    "    \"dropout\": 0.3,\n",
    "    # learning rate\n",
    "    \"lr\": 0.001,\n",
    "    \"max_length\": 100 \n",
    "}\n",
    "# input word embedding size which should be the times number of head_num\n",
    "config[\"d_model\"]=config[\"head_num\"]*50\n",
    "# hidden_size of hidden layers in MLP\n",
    "config[\"hidden_size\"]=2*config[\"d_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 40, 400])\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(config[\"batch_size\"],config[\"src_length\"],config[\"d_model\"])\n",
    "b=torch.ones(config[\"batch_size\"],config[\"trg_length\"],config[\"d_model\"])\n",
    "c=torch.zeros(config[\"d_model\"])\n",
    "a[:,-2:]=c\n",
    "b[:,-3:]=c\n",
    "# tmp1=(a.sum(dim=2)==0)\n",
    "# tmp2=(b.sum(dim=2)==0)\n",
    "# mask=get_attn_pad_mask(tmp1,tmp1)\n",
    "# mask=get_attn_pad_mask(tmp2,tmp2)\n",
    "# mask=get_attn_pad_mask(tmp2,tmp1)\n",
    "# print(mask.size())\n",
    "# print(mask[0])\n",
    "Tran=Transformer(config)\n",
    "res=Tran(a,b)\n",
    "print(res.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
